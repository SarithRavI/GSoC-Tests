{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec24f6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import h5py\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torchmetrics.classification import MulticlassAUROC, MulticlassAccuracy\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c018fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clearing cuda cache memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a8deab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['QCDToGGQQ_IMGjet_RH1all_jet0_run0_n36272',\n",
       " 'QCDToGGQQ_IMGjet_RH1all_jet0_run0_n36272.test.snappy.parquet',\n",
       " 'QCDToGGQQ_IMGjet_RH1all_jet0_run1_n47540',\n",
       " 'QCDToGGQQ_IMGjet_RH1all_jet0_run1_n47540.test.snappy.parquet',\n",
       " 'QCDToGGQQ_IMGjet_RH1all_jet0_run2_n55494',\n",
       " 'QCDToGGQQ_IMGjet_RH1all_jet0_run2_n55494.test.snappy.parquet',\n",
       " 'QG_Jets',\n",
       " 'quark-gluon_data-set_n139306.hdf5',\n",
       " 'SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5',\n",
       " 'SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset directory\n",
    "# this directory contains all the datasets related for ML4SCI tests.\n",
    "os.listdir(\"../dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "736e0247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "\n",
    "# importing electron dataset and seperating images and labels\n",
    "electron_dataset = h5py.File(\"../dataset/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\",\"r\")\n",
    "electron_imgs=np.array(electron_dataset[\"X\"])\n",
    "electron_labels=np.array(electron_dataset[\"y\"],dtype=np.int64)\n",
    "\n",
    "# importing photon dataset and seperating images and labels\n",
    "photon_dataset = h5py.File(\"../dataset/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5\",\"r\")\n",
    "photon_imgs=np.array(photon_dataset[\"X\"])\n",
    "photon_labels=np.array(photon_dataset[\"y\"],dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38ee333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate electron and photon images/labels\n",
    "img_arrs = torch.Tensor(np.vstack((photon_imgs,electron_imgs)))\n",
    "labels = torch.Tensor(np.hstack((photon_labels,electron_labels))).to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbf34d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([498000, 32, 32, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# images array shape\n",
    "img_arrs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a7854ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class\n",
    "# this will ease image/label reading at runtime\n",
    "class SingleElectronPhotonDataset(Dataset):\n",
    "    def __init__(self,split_inx, transform=None,target_transform= None):\n",
    "        self.img_arrs_split = img_arrs[split_inx]\n",
    "        self.labels_split = labels[split_inx]\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    def __len__(self):\n",
    "        return self.labels_split.shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        image=self.img_arrs_split[idx,:,:,:]\n",
    "        # changing the dim of image to channels, height, width by transposing the\n",
    "        # original image tensor.\n",
    "        image = image.permute(2,1,0)\n",
    "        label = self.labels_split[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a6dd41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,bias=False):\n",
    "        \"\"\"\n",
    "        Seperable convolution layer in Xception model, as specified in\n",
    "        https://arxiv.org/pdf/1610.02357.pdf\n",
    "        \"\"\"\n",
    "        super(SeparableConv2d,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,groups=in_channels,bias=bias)\n",
    "        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,reps,strides=1,start_with_relu=True,expand_first=True):\n",
    "        '''\n",
    "        reps: total number of separable conv layers in the block\n",
    "              note that separable conv layers are preceded by relu and followed batch normalization.\n",
    "        start_with_relu: if true start with relu \n",
    "        expand_first: if True latent embedding dim of the block will be expanded to out_channels \n",
    "                      at the beginning  else latent dim will be expanded at the end    \n",
    "        '''\n",
    "        super(Block, self).__init__()\n",
    "\n",
    "        if out_channels != in_channels or strides!=1:\n",
    "            self.skip = nn.Conv2d(in_channels,out_channels,1,stride=strides, bias=False)\n",
    "            self.skipbn = nn.BatchNorm2d(out_channels)\n",
    "        else:\n",
    "            self.skip=None\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        rep=[]\n",
    "\n",
    "        filters=in_channels\n",
    "        if expand_first:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(in_channels,out_channels,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(out_channels))\n",
    "            filters = out_channels\n",
    "\n",
    "        for i in range(reps-1):\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(filters))\n",
    "        \n",
    "        if not expand_first:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(in_channels,out_channels,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        if not start_with_relu:\n",
    "            rep = rep[1:]\n",
    "        else:\n",
    "            rep[0] = nn.ReLU(inplace=False)\n",
    "\n",
    "        if strides != 1:\n",
    "            rep.append(nn.MaxPool2d(3,strides,1))\n",
    "        self.rep = nn.Sequential(*rep)\n",
    "\n",
    "    def forward(self,inp):\n",
    "        x = self.rep(inp)\n",
    "\n",
    "        if self.skip is not None:\n",
    "            skip = self.skip(inp)\n",
    "            skip = self.skipbn(skip)\n",
    "        else:\n",
    "            skip = inp\n",
    "\n",
    "        x+=skip\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Xception(nn.Module):\n",
    "    \"\"\"\n",
    "    Xception model, as specified in\n",
    "    https://arxiv.org/pdf/1610.02357.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            num_classes: number of classes\n",
    "        \"\"\"\n",
    "        super(Xception, self).__init__()\n",
    "\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 32, 3,2, 0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32,64,3,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.block1=Block(64,128,2,2,start_with_relu=False,expand_first=True)\n",
    "        self.block2=Block(128,256,2,2,start_with_relu=True,expand_first=True)\n",
    "        self.block3=Block(256,728,2,2,start_with_relu=True,expand_first=True)\n",
    "\n",
    "        self.block4=Block(728,728,3,1,start_with_relu=True,expand_first=True)\n",
    "        self.block5=Block(728,728,3,1,start_with_relu=True,expand_first=True)\n",
    "        self.block6=Block(728,728,3,1,start_with_relu=True,expand_first=True)\n",
    "        self.block7=Block(728,728,3,1,start_with_relu=True,expand_first=True)\n",
    "        self.block8=Block(728,728,3,1,start_with_relu=True,expand_first=True)\n",
    "        self.block9=Block(728,728,3,1,start_with_relu=True,expand_first=True)\n",
    "        self.block10=Block(728,728,3,1,start_with_relu=True,expand_first=True)\n",
    "        self.block11=Block(728,728,3,1,start_with_relu=True,expand_first=True)\n",
    "\n",
    "        self.block12=Block(728,1024,2,2,start_with_relu=True,expand_first=False)\n",
    "\n",
    "        self.conv3 = SeparableConv2d(1024,1536,3,1,1)\n",
    "        self.bn3 = nn.BatchNorm2d(1536)\n",
    "\n",
    "        self.conv4 = SeparableConv2d(1536,2048,3,1,1)\n",
    "        self.bn4 = nn.BatchNorm2d(2048)\n",
    "\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.block7(x)\n",
    "        x = self.block8(x)\n",
    "        x = self.block9(x)\n",
    "        x = self.block10(x)\n",
    "        x = self.block11(x)\n",
    "        x = self.block12(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return F.softmax(x,dim=1)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Xception\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99298068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the device and the loss function\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "multicls_criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32f58392",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Xception(num_classes=2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ea2fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(96),\n",
    "    transforms.Normalize(mean=[0.5, 0.5], std=[0.5, 0.5]),\n",
    "])\n",
    "\n",
    "# random split of train, validation, tests set\n",
    "# seed it set to 42 for reproducability of results\n",
    "train_inx, valid_inx, test_inx = random_split(range(labels.shape[0]),[0.7,0.2,0.1],generator=torch.Generator()\n",
    "                                            .manual_seed(42))\n",
    "\n",
    "train_data = SingleElectronPhotonDataset(split_inx=train_inx,transform = preprocess)\n",
    "valid_data = SingleElectronPhotonDataset(split_inx=valid_inx,transform = preprocess)\n",
    "test_data = SingleElectronPhotonDataset(split_inx=test_inx,transform = preprocess)\n",
    "\n",
    "# data loaders \n",
    "train_dataloader = DataLoader(train_data,batch_size = 64, shuffle = True)\n",
    "valid_dataloader = DataLoader(valid_data,batch_size = 64, shuffle = True)\n",
    "test_dataloader = DataLoader(test_data,batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b69e7c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "def train(model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    loss_accum = 0\n",
    "    for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        output = model(inputs)\n",
    "        optimizer.zero_grad()\n",
    "        loss = multicls_criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_accum += loss.item()\n",
    "        \n",
    "    return loss_accum / (step + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0440fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation loop\n",
    "\n",
    "def evaluate(model, device, loader,evaluator= \"roauc\",isTqdm=False,returnLoss=False):\n",
    "    model.eval()\n",
    "    \n",
    "    preds_list = []\n",
    "    target_list = []\n",
    "    loss_accum =0 \n",
    "    iterator = enumerate(loader)\n",
    "    if isTqdm:\n",
    "        iterator = enumerate(tqdm(loader))\n",
    "    for step, batch in iterator:\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "            preds_list.extend(output.tolist())\n",
    "            if returnLoss:\n",
    "                loss = multicls_criterion(output, labels)\n",
    "                loss_accum += loss.item()        \n",
    "        target_list += batch[1].tolist()\n",
    "    if evaluator == \"roauc\":   \n",
    "        metric = MulticlassAUROC(num_classes=2, average=\"macro\", thresholds=None)\n",
    "    if evaluator == \"acc\":\n",
    "        metric = MulticlassAccuracy(num_classes=2, average=\"macro\")\n",
    "    # print(\"AUC-ROC metric score : \",metric(torch.Tensor(preds_list),torch.Tensor(target_list)).item())\n",
    "    return metric(torch.Tensor(preds_list),torch.Tensor(target_list).to(torch.int64)).item(),loss_accum/(step+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1996a54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup for loading and saving checkpoints\n",
    "checkpoints_path = \"../models\"\n",
    "checkpoints = os.listdir(checkpoints_path)\n",
    "checkpoint_path = list(filter(lambda i : (str(model) in i) and (len(i.split(\"-\"))==2), checkpoints))\n",
    "checkpoint_path = sorted(checkpoint_path,key=lambda n : [int(n[:-3].split(\"-\")[1]),n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2df7989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup for curve plotting using tensorboard\n",
    "curves_path = \"../tensorboard-plots\"\n",
    "writer = SummaryWriter(log_dir = f\"{curves_path}/{str(model)}/exp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d0d291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting maximum patience for early stopping\n",
    "\n",
    "maxPatience = 3 # patience for monotonic increase \n",
    "maxTolerance =5 # patience for gradual increase "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ea4930",
   "metadata": {},
   "source": [
    "### Training and evaluating the Xception model.\n",
    "\n",
    "#### Refer the [readme](https://github.com/SarithRavI/GSoC-Tests/tree/master/Task_1#readme) for performance analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5755b09d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Epoch 1\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 5447/5447 [43:23<00:00,  2.09it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "ROC-AUC scores:  {'Train': 0.4802662134170532, 'Validation': 0.4801178574562073}\n",
      "=====Epoch 2\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 5447/5447 [45:57<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "ROC-AUC scores:  {'Train': 0.7743293046951294, 'Validation': 0.7717928886413574}\n",
      "=====Epoch 3\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 5447/5447 [46:39<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "ROC-AUC scores:  {'Train': 0.6000425815582275, 'Validation': 0.5959718823432922}\n",
      "=====Epoch 4\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 5447/5447 [47:15<00:00,  1.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "ROC-AUC scores:  {'Train': 0.5411525964736938, 'Validation': 0.5417307615280151}\n",
      "=====Epoch 5\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 5447/5447 [46:33<00:00,  1.95it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "ROC-AUC scores:  {'Train': 0.7988113760948181, 'Validation': 0.7960073947906494}\n",
      "=====Epoch 6\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 5447/5447 [47:25<00:00,  1.91it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "ROC-AUC scores:  {'Train': 0.6784870624542236, 'Validation': 0.6708086729049683}\n",
      "=====Epoch 7\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 5447/5447 [48:45<00:00,  1.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "ROC-AUC scores:  {'Train': 0.559593915939331, 'Validation': 0.5561144948005676}\n",
      "=====Epoch 8\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 5447/5447 [45:44<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "ROC-AUC scores:  {'Train': 0.7494600415229797, 'Validation': 0.747252345085144}\n",
      "=====Epoch 9\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 5447/5447 [47:55<00:00,  1.89it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "ROC-AUC scores:  {'Train': 0.7898080348968506, 'Validation': 0.7854228019714355}\n",
      "=====Epoch 10\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 5447/5447 [47:11<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "ROC-AUC scores:  {'Train': 0.7997227907180786, 'Validation': 0.7943601608276367}\n",
      "=====Epoch 11\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 5447/5447 [38:00<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "ROC-AUC scores:  {'Train': 0.7981902360916138, 'Validation': 0.7911926507949829}\n",
      "=====Epoch 12\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 5447/5447 [47:20<00:00,  1.92it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "ROC-AUC scores:  {'Train': 0.7935458421707153, 'Validation': 0.7864241003990173}\n",
      "=====Epoch 13\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 5447/5447 [45:58<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "ROC-AUC scores:  {'Train': 0.7949322462081909, 'Validation': 0.7859148383140564}\n",
      "=====Epoch 14\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 5447/5447 [47:59<00:00,  1.89it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "ROC-AUC scores:  {'Train': 0.8055897951126099, 'Validation': 0.7946267127990723}\n",
      "=====Epoch 15\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 5447/5447 [42:19<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "ROC-AUC scores:  {'Train': 0.8171241283416748, 'Validation': 0.7988213300704956}\n",
      "=====Epoch 16\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 5447/5447 [41:37<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "ROC-AUC scores:  {'Train': 0.8195126056671143, 'Validation': 0.7981994152069092}\n",
      "=====Epoch 17\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 5447/5447 [40:39<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "ROC-AUC scores:  {'Train': 0.8239861726760864, 'Validation': 0.7992955446243286}\n",
      "=====Epoch 18\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 5447/5447 [41:41<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "ROC-AUC scores:  {'Train': 0.8264161348342896, 'Validation': 0.7949784398078918}\n",
      "=====Epoch 19\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 5447/5447 [41:42<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "ROC-AUC scores:  {'Train': 0.815655529499054, 'Validation': 0.7794774770736694}\n",
      "=====Epoch 20\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 5447/5447 [44:18<00:00,  2.05it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "ROC-AUC scores:  {'Train': 0.8242670297622681, 'Validation': 0.790569543838501}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished training!\n",
      "\n",
      "ROC-AUC Test score at last epoch: 0.7918810844421387\n",
      "\n",
      "Max ROC-AUC Validation score: 0.7992955446243286\n",
      "\n",
      "Max ROC-AUC Validation epoch: 17\n",
      "\n",
      "ROC-AUC Test score at epoch 17 : 0.7994509935379028\n"
     ]
    }
   ],
   "source": [
    "# list of values used for plotting\n",
    "train_losses = [1000]\n",
    "val_losses = [1000]\n",
    "\n",
    "# init values for early stopping and plotting\n",
    "currentPatience = 0\n",
    "currentTolerance = 0\n",
    "toleranceValidScore = -1000.0\n",
    "starting_epoch = 1\n",
    "max_val_epoch = 0\n",
    "max_val = 0\n",
    "# loading previous checkpoints\n",
    "if len(checkpoint_path)>0:\n",
    "    checkpoint = torch.load(f\"{checkpoints_path}/{checkpoint_path[-1]}\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    starting_epoch = checkpoint['epoch']+1\n",
    "    currentPatience = checkpoint['currentPatience']\n",
    "    maxPatience = max(checkpoint['prevMaxPatience'],maxPatience)\n",
    "    currentTolerance = checkpoint['currentTolerance']\n",
    "    maxTolerance = max(checkpoint['prevMaxTolerance'],maxTolerance)\n",
    "    toleranceValidScore = checkpoint['toleranceValidScore']\n",
    "    val_losses = [checkpoint['val_loss']]\n",
    "    train_losses = [checkpoint['train_loss']]\n",
    "    max_val_epoch = checkpoint['max_val_epoch']\n",
    "    max_val = checkpoint['max_val']\n",
    "\n",
    "# training\n",
    "for epoch in range(starting_epoch, epochs + 1):\n",
    "    print(\"=====Epoch {}\".format(epoch))\n",
    "    print('Training...')\n",
    "    train_loss = train(model, device, train_dataloader, optimizer)\n",
    "        \n",
    "    print(\"Evaluating...\")\n",
    "    train_perf_auc,_ = evaluate(model,device,train_dataloader,returnLoss=False)\n",
    "    valid_perf_auc,val_loss = evaluate(model,device,valid_dataloader,returnLoss=True)\n",
    "    \n",
    "    # keep the maximum val auc and the epoch \n",
    "    if max_val < valid_perf_auc:\n",
    "        max_val_epoch = epoch\n",
    "        max_val = valid_perf_auc\n",
    "        \n",
    "    if currentTolerance >0:\n",
    "        if toleranceValidScore <= val_loss:\n",
    "            currentTolerance+=1\n",
    "        else:\n",
    "            '''\n",
    "            Removed deleting\n",
    "            '''\n",
    "            # tolerancePoint = f\"{checkpoints_path}/{str(model)}-{epoch-(currentTolerance+1)}.pt\"\n",
    "            # os.remove(tolerancePoint)\n",
    "            currentTolerance=0 \n",
    "            \n",
    "    if train_losses[-1]>train_loss and val_losses[-1]<=val_loss:\n",
    "        currentPatience +=1\n",
    "        if currentTolerance == 0:\n",
    "            toleranceValidScore = val_losses[-1] # set the starting point a.k.a tolerance point\n",
    "            currentTolerance+=1\n",
    "    else: \n",
    "        '''\n",
    "        Removed deleting\n",
    "        '''\n",
    "        # for pre_inx in range(2,currentPatience+2):\n",
    "            # f = f\"{checkpoints_path}/{str(model)}-{epoch-pre_inx}.pt\"\n",
    "            # if (currentTolerance != pre_inx) and os.path.exists(f) :\n",
    "                # os.remove(f)\n",
    "        currentPatience = 0\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    writer.add_scalars('Loss', {\"train\" : train_loss,\n",
    "                              \"validation\"  : val_loss}, epoch)\n",
    "    writer.add_scalars(\"AUC\",{'train':train_perf_auc,\n",
    "                            'validation':valid_perf_auc}, epoch)\n",
    "\n",
    "    # print('Losses: ',{'Train': train_loss, 'Validation': val_loss})\n",
    "    print('ROC-AUC scores: ',{'Train': train_perf_auc, 'Validation': valid_perf_auc})\n",
    "    \n",
    "    # stopping if overfitting\n",
    "    \n",
    "    # stop if the the val loss has increased monotonically\n",
    "    if currentPatience == maxPatience:\n",
    "        print(\"Early stopping training due to overfitting...\")\n",
    "        print(f\"obtain results of epoch {epoch-maxPatience}\")\n",
    "        break\n",
    "    \n",
    "    # stop if the val loss has increased surpassing the tolerance patience\n",
    "    if currentTolerance == maxTolerance:\n",
    "        print(\"Early stopping training due to overfitting...\")\n",
    "        print(f\"obtain results of epoch {epoch-(currentTolerance)}\")\n",
    "        break\n",
    "    \n",
    "    # save checkpoint of current epoch\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'train_loss':train_loss,\n",
    "            'val_loss':val_loss,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'currentPatience':currentPatience,\n",
    "            'prevMaxPatience':maxPatience,\n",
    "            'currentTolerance':currentTolerance,\n",
    "            'prevMaxTolerance':maxTolerance,\n",
    "            'toleranceValidScore':toleranceValidScore,\n",
    "            'max_val_epoch':max_val_epoch,\n",
    "            'max_val':max_val\n",
    "            }, f\"{checkpoints_path}/{str(model)}-{epoch}.pt\")\n",
    "        \n",
    "print('\\nFinished training!')\n",
    "\n",
    "print('\\nROC-AUC Test score at last epoch: {}'.format(evaluate(model,device,test_dataloader)[0]))\n",
    "\n",
    "print('\\nMax ROC-AUC Validation score: {}'.format(max_val))\n",
    "print('\\nMax ROC-AUC Validation epoch: {}'.format(max_val_epoch))\n",
    "\n",
    "# loading the model with max val\n",
    "maxVal_checkpoint = torch.load(f\"{checkpoints_path}/{str(model)}-{max_val_epoch}.pt\")\n",
    "model.load_state_dict(maxVal_checkpoint['model_state_dict'])\n",
    "\n",
    "print('\\nROC-AUC Test score at epoch {} : {}'.format(max_val_epoch,evaluate(model,device,test_dataloader)[0]))\n",
    "\n",
    "# logging and plotting\n",
    "\n",
    "if currentPatience == maxPatience:\n",
    "    model_file = f\"{checkpoints_path}/{str(model)}-{epoch-maxPatience}.pt\"\n",
    "    if os.path.exists(model_file):\n",
    "        pre_model = torch.load(model_file)['model_state_dict']\n",
    "        model.load_state_dict(pre_model)\n",
    "        test_roc,test_loss = evaluate(model,device,test_dataloader,returnLoss=True)\n",
    "        print('\\nROC-AUC Test score in {} prior to overfitting: {}'.format(epoch-maxPatience,\n",
    "                                                                           test_roc))\n",
    "        print('\\nTest loss in {} prior to overfitting: {}'.format(epoch-maxPatience,\n",
    "                                                                           test_loss))\n",
    "\n",
    "elif currentTolerance == maxTolerance:\n",
    "    model_file = f\"{checkpoints_path}/{str(model)}-{epoch-maxTolerance}.pt\"\n",
    "    if os.path.exists(model_file):\n",
    "        pre_model = torch.load(model_file)['model_state_dict']\n",
    "        model.load_state_dict(pre_model)\n",
    "        test_roc,test_loss = evaluate(model,device,test_dataloader,returnLoss=True)\n",
    "        print('\\nROC-AUC Test score in {} prior to overfitting: {}'.format(epoch-maxTolerance,\n",
    "                                                                           test_roc))\n",
    "        print('\\nTest loss in {} prior to overfitting: {}'.format(epoch-maxTolerance,\n",
    "                                                                           test_loss))\n",
    "        \n",
    "writer.flush()\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
